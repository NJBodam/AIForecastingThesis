# Hybrid Neural Networks for Inflation Forecasting Prediction: The Impact of Feature Engineering

The project benchmarks variants of a Convolutional Neural Network (CNN) models (including Hybrids) against popular traditional forecasting models (ARIMA, SARIMA & VAR) in forecasting Germany’s CPI using an array of macroeconomic variables, under different forecasting methodologies; Recursive 1-step methodology (RM) and Multi-step methodology (MM). The purpose is to assess effectiveness of the CNN models and the impact of feature engineering in its predictive performance in modeling inflation. To accomplish this, we implemented variant ConvLSTM models using 1D, 2D, variational autoencoders (VAEs), Shapley additive explanations (SHAP) and extended lookback feature length (guided by the Partial Autocorrelation Function - PACF).

The results obtained are informative. Given the peculiarity of Germany’s CPI, ConvLSTM model predictive accuracy are better with !D models having extended lookback guided by PACF. Forecasting methodology and context is also a key factor to be considered when benchmarking traditional models against deep learning models as models like SARIMA and ARIMA are inherently recursive.


## Overview

This repository serves as the codebase for research and experiments on advanced forecasting techniques using AI. The work utilizes various machine learning algorithms—especially deep learning models—to forecast time series data such as economic indicators, demand, and more.

Initially, it should be noted there are five notebooks with different purposes. The first one in order of use, called "data_processing", is responsible for preparing the raw input data. Essentially, this notebook receives the macroeconomic time series and run the necessary data manipulation routines. The VAEs and overlapping K-fold subsampling technique are implemented here as well. The outputs are used in the “cpi_inflation_MM_forecasting" and “cpi_inflation_RM_forecasting” notebooks, which contains the implemented models and adjusts every model using the outputs of the “data_processing” notebook based not the forecasting methodologies of Multi-step and Recursive 1-step respectively. 

The notebooks “results_MM_forecasting ” and “results_RM_forecasting ” compiles the forecasts yielded by each model under their respective methodology implementation (cpi_inflation_MM_forecasting" and “cpi_inflation_RM_forecasting)     and provides them as output for the user along with performance metrics.

--- Directories
In both notebooks, in the beginning of the code, the user can find string variables containing multiple directories. Briefly, some of these directories should contain the input data for the notebooks, while the others are going to store the outputs generated by the routines.
In the Inflation_Data notebook, the directories and file names are the following:


## Directories
In data_processing the directory and file names are:
- file_path —> project working directory 
- t_Data_Dir —> folder for storing transformed preprocessed data
- data —> file containing raw data input. Also publicly accessible here 
- all_Series_CPI_target_dir —> folder containing overlapping subsamples of the transformed series stored in t_Data_Dir for robust testing

In cpi_inflation_MM_forecasting and cpi_inflation_MM_forecasting the directory and file names are:
- output_dir -> directory for storing model results for the corresponding forecasting methodology
- mae_df -> dataframe containing all model MAE metric results for the corresponding forecasting methodology
- rmse_df -> dataframe containing all model RMSE metric results for the corresponding forecasting methodology
- forecast_df -> dataframe containing all models predicted and actual values for the corresponding forecasting methodology


## Project Structure

```
.
├── cpi_inflation_MM_forecasting.ipynb           # Python scripts and utility functions used for training and testing for MM
├── cpi_inflation_RM_forecasting.ipynb           # Python scripts and utility functions used for training and testing for RM
├── data_processing.ipynb                 	      # Python scripts used for preprocessing the data 
├── README.md            				             # Project documentation
├── results_MM_forecasting                 	      # Output results and figures for MM
└── results_RM_forecasting      				      # Output results and figures for RM
```

## Notebook - data_processing

The main input data to start running the code are selectively chosen from four sources the ECB data portal, Deutsche Bundesbank, Destatis and MarketWatch based on the interval (monthly) and length of the series. The same data used in the project can be accessed freely at https://drive.google.com/file/d/11VbOaLsHRLWok6Az5ninMqHx33zsdrW4/view?usp=sharing

Data Preprocessing Steps:
	•	Data Loading: Reading the time series data from a CSV file (e.g., data.csv).
	•	Log Transformation and Differencing: Applying logging given scale and differencing for stationarity.
	•	Granger Causality Test: To identify most significant explanatory variables for the VAR model implementation
	•	Stationarity Testing: Ensuring series are stationary using KPSS test.
	•	Variational Autoencoders: Generating a latent dimensional representation of the original series.

	•	Concatenation and Subsampling: Concatenating VAE series and transformed series into single csv and creating overlapping subsamples for robust testing.


	•	Reshaping for CNN-LSTM: Reshaping the input data into a 5D tensor format (samples, time_steps, rows, cols, features) suitable for ConvLSTM2D layers. In this project, time_steps, rows, and cols are set to 1 for simplicity, focusing on feature dimension.


## Notebook - cpi_inflation_MM_forecasting / cpi_inflation_RM_forecasting

The output from data_processing is then used to train and test the respective models in this section.
The steps involved here:
* Data Loading: Loading data (preprocessing data)
* Utility functions definition: functions needed by models
* VAR model optimum lag estimation: using the AIC criterion.
* VAR model implementation: done using only the significant “granger-caused” variables of the transformed dataset
* ARIMA model optimum lag estimation: using the AIC criterion.
* ARIMA model implementation: using the transformed dataset.
* SARIMA model optimum lag estimation: using the AIC criterion.
* SARIMA model implementation: using the transformed dataset.
* ConvLSTM input data reshaping: !D, 2D and time feature engineering
* ConvLSTM variant implementation: using the transformed dataset.
* Shapley Additive Implementation: as a variable selection technique for the ConvLSTM-SHAP model
* ConLSTM-SHAP implementation: using the transformed dataset
* Robust Test: using generated overlapping subsamples as a form of K-fold cross validation.

## Notebook - results_MM_forecasting / results_RM_forecasting

* Data Loading: output from cpi_inflation_MM_forecasting / cpi_inflation_RM_forecasting
* Utility functions definition: functions for graphs and tables.
* Plots: various metric and graphic plot.
* Signicance Testing: using Wilcoxon Pairwise test and others.


## Key Features

- Implementation of classical and modern forecasting models (ARIMA, SARIMA, ConvLSTM, etc.)
- Data preprocessing and feature engineering for time series
- Robust testing with samples derived using overlapping K-fold cross validation technique
- Model evaluation and comparison
- Visualization of forecasts and model performance
- Reproducible Jupyter notebooks

## Models Implemented

This repository contains implementations and evaluations of the following time series forecasting models:
	1	Vector Autoregression (VAR): A classical multivariate time series model used as a benchmark.
	2	Autoregressive Integrated Moving Average (ARIMA): A univariate time series model adapted for a single variable forecast in this context.
	3	Seasonal Autoregressive Integrated Moving Average (SARIMA): An extension of ARIMA to handle seasonality in time series data.
	4	Variant Convolutional Neural Network: A deep learning model that combines convolutional layers for feature extraction. We employed various implementations  such as 1D, 2D, variational autoencoders (VAEs), Shapley additive explanations (SHAP) and extended lookback feature length (guided by the Partial Autocorrelation Function - PACF) 




